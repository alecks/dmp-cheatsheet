\section{Probability}

\textbf{Bold text \textit{anywhere} signifies truth for both discrete and continuous RVs.} If bold but not explicit, change \(p(x)\) to \(f(x)\) or integral to a sum, v.v.

\subsection*{Counting}

\begin{itemize}
	\item Experiment 1 has \(n\) outcomes, another has \(m\). This gives \(n \cdot m\) outcomes. \(N\) repitions of Experiment 1 gives \(n^N\) outcomes. 
	%
	\item Permutations: how many ways to order a set. \(n!\), or if we count repeated items as indistinguishable, \(\frac{n!}{r_1! \cdot \cdots \cdot r_i!}\) where \(r_i\) is the number of times the number \(i\) was repeated.
	%
	\item \(P_{n,k} = \frac{n!}{(n-k)!}\) (\(\binom{n}{k}\) but order matters), \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\)
	%
	\item The number of ways to choose something OR something else is addition. Choosing two things together (AND) is multiplication. If we are to find `at least one (up to 3)', it would be the number of ways to choose 1 OR 2 OR 3.
\end{itemize}


\subsection*{Basic Probability}

\begin{itemize}
	\item \(\bigcup_{i} E_i\) means at least one \(E_i\) and \(\bigcap_{i} E_i\) means all of the \(E_i\)'s.
	%
	\item Mutex events (disjoint) can't happen at the same time. If \(E \subseteq F\), \(E\) can't happen without \(F\) happening. \(E^c = \Omega - E\).
	%
	\item Axioms: for countably many \textit{mutex} events, \(P(\bigcup E_i) = \sum_i P(E_i)\). If not mutex, this equality is replaced with \(\leq\).
	%
	\item Elementary events are those such that all events have probability \(\frac{1}{|\Omega|}\).
	%
	\item \(P(F-E) = P(F \cap E^c) = P(F)-P(F \cap E)\). This is the set difference law.
	%
	\item De Morgan's: \((E \cap F)^c = E^c \cup F^c\) (flip intersection/union and take complement).
	%
	%
	\item \(P(E|F) = \frac{P(E \cap F)}{P(F)}\). All axioms work fine, just add the `given' part to each.
	%
	\item Multiplication rule: \(P(E_1 \cap E_2 \cap \cdots \cap E_n) = P(E_1) \cdot P(E_2 | E_1) \cdot P(E_3 | E_1 \cap E_2) \cdots P(E_n | E_1 \cap \cdots \cap E_{n-1})\).
	%
	\item \textit{Events} are independent \(\iff P(F|E) = P(F) \iff P(E|F) = P(E) \iff P(E \cap F) = P(E) \cdot P(F)\). \textbf{Random variables} are thus independent \(\iff p_{Y|X}(y|x)=p_Y(y) \iff p_{X|Y}(x|y)=p_X(x) \iff p(x,y) = p_X(x) \cdot p_Y(y)\) for all \((x,y)\).
	%
	\item Three or more events are mutually independent if the above multiplication rule applies to all pairs, and all of them together.
	%
	\item Murphy's Law: As \(n \to \infty\) with fixed probability \(p\), \(P(\text{all \(n\) experiments succeed})=p^n \to 0\), \(P(\text{at least one succeeds})=1-P(\text{each one fails})=1-(1-p)^n \to 1\), \(P(\text{exactly \(k\) succeed})=\binom{n}{k} \cdot p^k \cdot (1-p)^{n-k}\).
\end{itemize}


\subsection*{Discrete Random Variables}

\begin{itemize}
	\item PMF: \(p_X(x)=P(X=x)\), CDF: \(F_X(x)=P(X \leq x) = \sum_{x_i \leq x} p(x_i)\). \textbf{CDF is non-decreasing.}
	%
	\item \(\mathbb{E}(X) = \sum_i x_i \cdot p(x_i)\). Needn't be a possible value. \textbf{Moments}: \(n\)th moment: \(\mathbb{E}(X^n)\). Absolute moment: \(\mathbb{E}(|X|^n)\).
	%
	\item \(\operatorname{Var}(X) = \mathbb{E}(X - \mathbb{E}(X))^2 = \mathbb{E}(X^2) - \mathbb{E}(X)^2\). \(\operatorname{SD}(X) = \sqrt{\operatorname{Var}(X)}\).
	%
	\item \textbf{Chebyshev Inequality}: for any constant \(k \geq 1\), the probability that \(X\) is more than \(k\) standard deviations away from the mean is no more than \(\frac{1}{k^2}\). \(P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}\).
	%
	\item Binomial: there are \(n\) independent trials, each succeeding with probability \(p\). \(X\) counts the number of successes: \(X \sim \operatorname{Bin}(n,p)\). If \(n=1\), this is Bernoulli where \(P(X=1)=p\) is the probability of the `only' success. \(p(x)=P(\text{number of successes is x}) = \binom{n}{x} p^x (1-p)^{n-x}\), \(\mathbb{E}(X)=np\), \(\operatorname{Var}(X)=np(1-p)\).
	%
	\item Poisson: \(X \sim \operatorname{Pois}(\lambda \in \mathbb{R}^+)\) if \(p(x)=e^{-\lambda} \cdot \frac{\lambda^x}{x!}\), \(\mathbb{E}(X)=\operatorname{Var}(X)=\lambda\). Poisson measures the probability of a number of events happening in a space, based on an average (\(\lambda\)). For example, number of calls per hour, or number of typos on a page. \(\operatorname{Bin}(n,p) \approx \operatorname{Pois}(np)\) for large \(n\) and small \(p\).
	%
	\item Geometric: \(X \sim \operatorname{Geom}(p)\) counts the number of independent trials repeated until we get a success (with probability \(p\)), with no memory. \(p(x)=p(1-p)^{x-1}\), \(\mathbb{E}(X)=\frac{1}{p}\), \(\operatorname{Var}(X)=\frac{1-p}{p^2}\).
\end{itemize}


\subsection*{Continuous Random Variables}

\begin{itemize}
\item The PDF is defined as being the function \(f(x)\) such that its integral, from \(a\) to \(b\), gives the probability \(P(a \leq x \leq b)\), and from \(-\infty\) to \(\infty\) gives \(1\). The derivative of the CDF is the PDF.
%
\item CDF: \(F(x) = P(X \leq x) = \int_{-\infty}^{x}f(y)\,dy\). There will usually be a lower bound for \(Y\), such that any values of \(Y\) less than this lower bound will have a \(0\) probability, meaning we don't have to compute an improper integral.
%
\item \(P(X > a)=1-F(a)\), \(P(a \leq X \leq b)=F(b)-F(a)\).
%
\item Percentiles: the 75th percentile is the value \(\eta_{0.75}\) s.t. \(P(X < \eta) = 0.75\). Thus we can calculate it with the inverse of the CDF: \(\eta_{0.5}=F^{-1}(0.5)\), which also happens to be the \textit{median}, sometimes written just \(\eta\).
%
\item \(\mathbb{E}(X) = \int_{-\infty}^{\infty} x \cdot f(x)\,dx = \mu_X\), \(\mathbb{E}(h(X)) = \int_{-\infty}^{\infty} h(x) \cdot f(x)\,dx\).
%
\item \(\operatorname{Var}(X) = \mathbb{E}((X - \mathbb{E}(X))^2) = \mathbb{E}(X^2) - \mu^2\), \(\operatorname{SD}(X) = \sigma_X = \sqrt{\operatorname{Var}(X)}\).
%
% 
\item Uniform: \(X \sim U(a,b)\) if \(f(x)=\frac{1}{b-a}\) if \(a \leq x \leq b\), \(f(x)=0\) otherwise. Equal probabilities for anything between \(a\) and \(b\), otherwise \(0\).
%
\item No real need for CDF. Use rectangle intuition: the height is \(\frac{1}{b-a}\) and the width would be the amount `along' the rectangle you would be. \(P(a \leq X \leq c) = (c-a) \cdot \frac{1}{b-a} = P(X \leq c)\) if the probability is equal between \(a\) and \(b\). If the density is high, the CDF's graph is steep.
%
\item Normal (Gaussian): \(X \sim N(\mu, \sigma)\) if \(f(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \cdot e^{-(x-\mu)^2 / (2\sigma^2)}\). \(\mu\) is the centre while \(\sigma\) measures how widely spread it is. Height, sheep producing wool, etc., where many random factors are involved.
%
\item About \(\frac{2}{3}\) of probability mass is within one SD of the mean, 95\% within two.
%
\item Standard normal if \(\mu=0\) and \(\sigma=1\). \(f(z; 0,1) = \frac{1}{\sqrt{2\pi}} \cdot e^{-z^2 / 2}\).
%
\item Standardising: If \(X \sim N(\mu, \sigma)\), \(Z=\frac{X-\mu}{\sigma} \mid Z \sim N(0,1)\). \(P(X \leq a) = P(Z \leq \frac{a-\mu}{\sigma}) = \Phi(\frac{a-\mu}{\sigma})\), \(\eta_p = \mu + \Phi^{-1}(p) \cdot \sigma\). Go opposite way in table for inverse.
%
\item Approximating binomial: find \(\mu = np\) and \(\sigma = \sqrt{npq}\) where \(q = (1-p)\). Use these two values as parameters for normal. Thus \(P(X \leq x) = \Phi(\frac{x + 0.5 - np}{\sqrt{npq}})\). Adequate if \(np,nq\geq 10\). We add \(0.5\) for continuity correction.
%
\item Exponential: how long until something happens, with no memory: \(X \sim \operatorname{Exp}(\lambda)\) if \(f(x; \lambda) = \lambda e^{-\lambda x}\) if \(x > 0\), \(0\) otherwise. \(F(x; \lambda) = 1-e^{-\lambda x}\) if \(x > 0\), \(0\) otherwise.
%
\item \(\mathbb{E}(X) = \frac{1}{\lambda} = \operatorname{SD}(X)\). \(\operatorname{Var}(X) = \frac{1}{\lambda^2}\).
%
\item Relation to Poisson: Poisson counts the number of arrivals each minute, while exponential counts the time between arrivals at a drive-through.
\end{itemize}

\subsection*{Transformations of Random Variables}

\begin{itemize}
	\item \textbf{Linearity of \(\mathbb{E}\)}: expectation of the sum of RVs is the sum of their expectations. \textbf{Rescaling}: \(\mathbb{E}(aX + b)=a \mathbb{E}(X) + b\), \(\operatorname{Var}(aX + b) = a^2 \operatorname{Var}(X)\), \(\operatorname{SD}(aX+b) = |a|\operatorname{SD}(X)\).
	%
	\item \(\mathbb{E}(h(X)) = \int_{-\infty}^{\infty} h(x) \cdot f(x)\,dx\), \(E(h(X)) = \sum_i h(x_i) \cdot p(x_i)\).
	%
	\item \(\operatorname{Cov}(aX+b,cY+d) = ac \operatorname{Cov}(X,Y)\), \(\operatorname{Cov}(aX+bY+c, Z) = a\operatorname{Cov}(X,Z) + b\operatorname{Cov}(Y,Z)\).
	%
	\item \textbf{Transformations of RVs are themselves RVs}: if a transformation of a random variable \(Y=g(X)\) is monotonically increasing, like radius to area, then CDF is \(F_Y(y)=F_X(g^{-1}(y))\). If monotonically decreasing, like speed to time, then \(F_Y(y)=1 - F_X(g^{-1}(y))\) since the inequality is flipped.
	%
	\item PDF is \(f_Y(y) = f_X(g^{-1}(y)) \cdot |\frac{d}{dy} g^{-1}(y)|\), where the derivative accounts for the change in width of the curve.
\end{itemize}

\subsection*{Joint Probability Distributions}

\subsubsection*{Discrete}

\begin{itemize}
	\item JPMF of \(X\) and \(Y\) is \(p(x,y)\) defined for every pair s.t. \(p(x,y)=P(X=x \land Y=y)\). For an event \(A \subset \mathbb{R} \times \mathbb{R}\), the probability of any of its outcomes occuring is \(P((X,Y) \in A) = \sum_{(x,y) \in A} p(x,y)\).
	%
	\item Marginal probabilities allow you to calculate individual variables' PMFs from their JPMF: \(p_X(x)=\sum_y p(x,y)\), \(p_Y(y)=\sum_x p(x,y)\). If given a table, add up the values in each column or row.
	%
	\item \(\mathbb{E}(g(X,Y)) = \sum_x \sum_y g(x,y) \cdot p(x,y)\).
\end{itemize}

\subsubsection*{Continuous}

\begin{itemize}
	\item Imagine a rectangle \(A = \{(x,y) \mid a \leq x \leq b, c \leq y \leq d\}\). The probability \(P((X,Y) \in A) = P(a \leq X \leq b, c \leq Y \leq d) = \int_a^b (\int_c^d f(x,y)\,dy)\,dx = \int_c^d (\int_a^b f(x,y)\,dx)\,dy\). This is the probability that the random variables lie in the same rectangle together.
	%
	\item Marginal probabilities: \(f_X(x) = \int_{-\infty}^{\infty} f(x,y)\,dy\), \(f_Y(y)=\int_{-\infty}^{\infty} f(x,y)\,dx\). If given a support, like \(a \leq X \leq b\) and \(c \leq Y \leq d\), use the bounds of the \textit{variable you're integrating with respect to} (which avoids improper integrals).
	%
	\item \(\mathbb{E}(g(X,Y)) = \int_{-\infty}^{\infty} (\int_{-\infty}^{\infty} g(x,y) \cdot f(x,y)\,dx)\,dy\).
\end{itemize}

\subsubsection*{Variance, Sums and Combinations}

\begin{itemize}
	\item \textbf{Linearity of expectation applies. Multiplying expectations of different random variables applies only if they are independent:} \(\mathbb{E}(h(X,Y)) = \mathbb{E}(g_1(X) \cdot g_2(Y)) = \mathbb{E}(g_1(X)) \cdot \mathbb{E}(g_2(Y))\), and \(h(X,Y)=g_1(X) \cdot g_2(X)\).
	%
	\item \textbf{Covariance}: how much do \(X\) and \(Y\) vary together? If \(\operatorname{Cov}(X, Y) > 0\), they vary together. If \(0\), they do not vary together. Vary in opposition otherwise. X and Y independent \(\implies \operatorname{Cov}(X,Y)=0\), but not the other way round.
	%
	\item \textbf{Definition of \(\operatorname{Cov}\)}: \(\operatorname{Cov}(X,Y)=\operatorname{Cov}(Y,X) = \mathbb{E}(XY)-\mu_X \cdot \mu_Y\), \(\operatorname{Cov}(X,X)=\operatorname{Var}(X)\).
	%
	\item \textbf{Correlation coefficient}: similar to standard deviation. \(\rho_{X,Y}=\operatorname{Corr}(X,Y)=\frac{\operatorname{Cov}(X,Y)}{\sigma_X \cdot \sigma_Y}\). If \(\rho > 0\), positively correlated. Not linearly correlated or negatively correlated otherwise.
	%
	\item \textbf{Properties of \(\operatorname{Corr}\)}: \(\operatorname{Corr}(X,Y)=\operatorname{Corr}(Y,X)\), \(\operatorname{Corr(X,X)=1}\) \(\operatorname{Corr}(X,Y)=0 \iff \mathbb{E}(XY) = \mu_X \cdot \mu_Y\), \(\operatorname{Corr}(X,Y)=1 \iff Y=aX+b \mid a > 0\), \(\operatorname{Corr}(X,Y)=-1 \iff Y=aX+b \mid a < 0\). Correlation has the range \([-1,1]\).
	%
	\item \textbf{Linear combinations} of expectations are trivial by the linearity of expectation. For variance: \(\operatorname{Var}(a_1 X_1 + \cdots + a_n X_n + b) = \sum_{i=1}^n \sum_{j=1}^n a_i a_j \operatorname{Cov}(X_i, X_j)\), \(\operatorname{Var}(aX+bY)=a^2 \operatorname{Var}(X) + b^2 \operatorname{Var}(Y) + 2ab \cdot \operatorname{Cov}(X,Y)\).
	%
	\item \textbf{With independence}: \(\operatorname{Var}(a_1 X_1 + \cdots + a_n X_n + b) = a_1^2 \operatorname{Var}(X_1) + \cdots + a_n^2 \operatorname{Var}(X_n)\), \(\operatorname{SD}(\cdots) = \sqrt{a_1^2 \sigma_1^2 + \cdots + a_n^2 \sigma_n^2}\), \(\operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) = \operatorname{Var}(X-Y)\).
	%
	\item \textbf{Sums of RVs}: Let \(W = X+Y\). \(f_W(w)=\int_{-\infty}^{\infty} f(x,w-x)\,dx\). If \(X\) and \(Y\) are independent, can integrate product of marginals. This is convolution: \(f_W = f_X \ast f_Y\). If discrete, sum instead.
	%
	\item \textbf{Sums of standard distributions}: If \(X_1 \cdots X_n\) are independent Poisson RVs with means \(\mu_1 \cdots \mu_n\), their sum is also Poisson with \(\mu = \mu_1 + \cdots + \mu_n\). Similar for normal distributions, with \(\sigma = \sqrt{\sigma_1^2 + \cdots + \sigma_n^2}\).
	%
\end{itemize}

\subsection*{Conditional and Limit Distributions}

\subsubsection*{Conditional}

\begin{itemize}
	\item \textbf{The probability that} \(Y=y\) given we know \(X=x\) is \(P_{Y|X}(y|x) = \frac{p(x,y)}{p_X(x)}\).
	%
	\item \textbf{Conditional mean}: \(\mu_{Y|X=x} = \mathbb{E}(Y|X = x) = \sum_y y \cdot p_{Y|X}(y|x)\), \(\mathbb{E}(Y|X = x) = \int_{-\infty}^{\infty} y \cdot f_{Y|X}(y|x)\,dy\). For a function \(h(y)\), replace \(y\) with \(h(y)\).
	%
	\item \textbf{Conditional variance}: \(\sigma_{Y|X=x}^2 = \operatorname{Var}(Y|X = x) = \mathbb{E}((Y - \mu_{Y|X=x})^2 | X=x) = \mathbb{E}(Y^2 | X=x) - \mu_{Y|X=x}^2\).
	%
	\item \textbf{Law of total expectation/variance:} \(\mathbb{E}(Y|X)\) and \(\operatorname{Var}(Y|X)\) are themselves random variables with their own mean and variance: \(\mathbb{E}(Y) = \mathbb{E}(\mathbb{E}(Y|X))\), \(\operatorname{Var}(Y) = \operatorname{Var}(\mathbb{E}(Y|X)) + \mathbb{E}(\operatorname{Var}(Y|X))\).
\end{itemize}

\subsubsection*{Limit}

\begin{itemize}
	\item If we have a \textbf{random sample}, a set of RVs with size \(n\) s.t. each RV is independent and identically distributed, the sample total \(T = \sum_{i=1}^n X_i\). The sample mean \(\bar{X} = \frac{T}{n}\). These are themselves RVs.
	%
	\item \textbf{Properties} of \(T\): \(\mathbb{E}(T)=n \cdot \mu\), where \(\mu\) is the underlying mean of the distribution and \(\sigma\) is the SD. \(\operatorname{Var}(T)=n\sigma^2\), \(\operatorname{SD}(T)=\sqrt{n} \cdot \sigma\). If the \(X_i\) are normally distributed, so is \(T\).
	%
	\item \textbf{Properties} of \(\bar{X}\): \(\mathbb{E}(\mu)\), \(\operatorname{Var}(\bar{X}) = \frac{\sigma^2}{n})\), \(\operatorname{SD}(\bar{X}) = \frac{\sigma}{\sqrt{n}}\). If the \(X_i\) are normal, so is \(\bar{X}\).
	%
	\item \textbf{Law of large numbers}: as \(n \to \infty\), the following are true: \(\bar{X}\) converges to \(\mu\); \(\mathbb{E}((\bar{X} - \mu)^2) \to 0\); \(P((\bar{X} - \mu) \geq \epsilon) \to 0\) for \(\epsilon > 0\).
	%
	\item \textbf{Central limit theorem}: summing distributions leads to a narrower and narrower distribution; with large \(n\), the less skewed the distribution gets, thus approaching normal. The peak of the resulting normal distrib. will be around the same mean \(\mu\).
	%
	\item \textbf{For IID RVs}, \(\lim_{n \to \infty} \bar{X} \sim N(n \mu, \frac{\sigma}{\sqrt{n}})\), and \(\lim_{n \to \infty} T \sim N(n \mu, \sqrt{n} \cdot \sigma)\).
	%
	\item We say that they are \textbf{asymptotically normal}.
\end{itemize}
