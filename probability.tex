\section{Probability}

\textbf{Bold text signifies truth for both discrete and continuous RVs.}

\subsection*{Counting}

\begin{itemize}
	\item Experiment 1 has \(n\) outcomes, another has \(m\). This gives \(n \cdot m\) outcomes. \(N\) repitions of Experiment 1 gives \(n^N\) outcomes. 
	%
	\item Permutations: how many ways to order a set. \(n!\), or if we count repeated items as indistinguishable, \(\frac{n!}{r_1! \cdot \cdots \cdot r_i!}\) where \(r_i\) is the number of times the number \(i\) was repeated.
	%
	\item \(P_{n,k} = \frac{n!}{(n-k)!}\) (\(\binom{n}{k}\) but order matters), \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\)
	%
	\item The number of ways to choose something OR something else is addition. Choosing two things together (AND) is multiplication. If we are to find `at least one (up to 3)', it would be the number of ways to choose 1 OR 2 OR 3.
\end{itemize}


\subsection*{Basic Probability}

\begin{itemize}
	\item \(\bigcup_{i} E_i\) means at least one \(E_i\) and \(\bigcap_{i} E_i\) means all of the \(E_i\)'s.
	%
	\item Mutex events (disjoint) can't happen at the same time. If \(E \subseteq F\), \(E\) can't happen without \(F\) happening. \(E^c = \Omega - E\).
	%
	\item Axioms: for countably many \textit{mutex} events, \(P(\bigcup E_i) = \sum_i P(E_i)\). If not mutex, this equality is replaced with \(\leq\).
	%
	\item Elementary events are those such that all events have probability \(\frac{1}{|\Omega|}\).
	%
	\item \(P(F-E) = P(F \cap E^c) = P(F)-P(F \cap E)\). This is the set difference law.
	%
	\item De Morgan's: \((E \cap F)^c = E^c \cup F^c\) (flip intersection/union and take complement).
	%
	%
	\item \(P(E|F) = \frac{P(E \cap F)}{P(F)}\). All axioms work fine, just add the `given' part to each.
	%
	\item Multiplication rule: \(P(E_1 \cap E_2 \cap \cdots \cap E_n) = P(E_1) \cdot P(E_2 | E_1) \cdot P(E_3 | E_1 \cap E_2) \cdots P(E_n | E_1 \cap \cdots \cap E_{n-1})\).
	%
	\item Independent \(\iff P(F|E) = P(F) \iff P(E|F) = P(E) \iff P(E \cap F) = P(E) \cdot P(F)\).
	%
	\item Three or more events are mutually independent if the above multiplication rule applies to all pairs, and all of them together.
	%
	\item Murphy's Law: As \(n \to \infty\) with fixed probability \(p\), \(P(\text{all \(n\) experiments succeed})=p^n \to 0\), \(P(\text{at least one succeeds})=1-P(\text{each one fails})=1-(1-p)^n \to 1\), \(P(\text{exactly \(k\) succeed})=\binom{n}{k} \cdot p^k \cdot (1-p)^{n-k}\).
\end{itemize}


\subsection*{Discrete Random Variables}

\begin{itemize}
	\item PMF: \(p_X(x)=P(X=x)\), CDF: \(F_X(x)=P(X \leq x) = \sum_{x_i \leq x} p(x_i)\). \textbf{CDF is non-decreasing.}
	%
	\item \(\mathbb{E}(X) = \sum_i x_i \cdot p(x_i)\). Needn't be a possible value. \textbf{Moments}: \(n\)th moment: \(\mathbb{E}(X^n)\). Absolute moment: \(\mathbb{E}(|X|^n)\).
	%
	\item \(\operatorname{Var}(X) = \mathbb{E}(X - \mathbb{E}(X))^2 = \mathbb{E}(X^2) - \mathbb{E}(X)^2\). \(\operatorname{SD}(X) = \sqrt{\operatorname{Var}(X)}\).
	%
	\item \textbf{Chebyshev Inequality}: for any constant \(k \geq 1\), the probability that \(X\) is more than \(k\) standard deviations away from the mean is no more than \(\frac{1}{k^2}\). \(P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}\).
	%
	\item Binomial: there are \(n\) independent trials, each succeeding with probability \(p\). \(X\) counts the number of successes: \(X \sim \operatorname{Bin}(n,p)\). If \(n=1\), this is Bernoulli where \(P(X=1)=p\) is the probability of the `only' success. \(p(x)=P(\text{number of successes is x}) = \binom{n}{x} p^x (1-p)^{n-x}\), \(\mathbb{E}(X)=np\), \(\operatorname{Var}(X)=np(1-p)\).
	%
	\item Poisson: \(X \sim \operatorname{Pois}(\lambda \in \mathbb{R}^+)\) if \(p(x)=e^{-\lambda} \cdot \frac{\lambda^x}{x!}\), \(\mathbb{E}(X)=\operatorname{Var}(X)=\lambda\). Poisson measures the probability of a number of events happening in a space, based on an average (\(\lambda\)). For example, number of calls per hour, or number of typos on a page. \(\operatorname{Bin}(n,p) \approx \operatorname{Pois}(np)\) for large \(n\) and small \(p\).
	%
	\item Geometric: \(X \sim \operatorname{Geom}(p)\) counts the number of independent trials repeated until we get a success (with probability \(p\)), with no memory. \(p(x)=p(1-p)^{x-1}\), \(\mathbb{E}(X)=\frac{1}{p}\), \(\operatorname{Var}(X)=\frac{1-p}{p^2}\).
\end{itemize}


\subsection*{Continuous Random Variables}

\begin{itemize}
\item The PDF is defined as being the function \(f(x)\) such that its integral, from \(a\) to \(b\), gives the probability \(P(a \leq x \leq b)\), and from \(-\infty\) to \(\infty\) gives \(1\). The derivative of the CDF is the PDF.
%
\item CDF: \(F(x) = P(X \leq x) = \int_{-\infty}^{x}f(y)\,dy\). There will usually be a lower bound for \(Y\), such that any values of \(Y\) less than this lower bound will have a \(0\) probability, meaning we don't have to compute an improper integral.
%
\item \(P(X > a)=1-F(a)\), \(P(a \leq X \leq b)=F(b)-F(a)\).
%
\item Percentiles: the 75th percentile is the value \(\eta_{0.75}\) s.t. \(P(X < \eta) = 0.75\). Thus we can calculate it with the inverse of the CDF: \(\eta_{0.5}=F^{-1}(0.5)\), which also happens to be the \textit{median}, sometimes written just \(\eta\).
%
\item \(\mathbb{E}(X) = \int_{-\infty}^{\infty} x \cdot f(x)\,dx = \mu_X\), \(\mathbb{E}(h(X)) = \int_{-\infty}^{\infty} h(x) \cdot f(x)\,dx\).
%
\item \(\operatorname{Var}(X) = \mathbb{E}((X - \mathbb{E}(X))^2) = \mathbb{E}(X^2) - \mu^2\), \(\operatorname{SD}(X) = \sigma_X = \sqrt{\operatorname{Var}(X)}\).
%
% 
\item Uniform: \(X \sim U(a,b)\) if \(f(x)=\frac{1}{b-a}\) if \(a \leq x \leq b\), \(f(x)=0\) otherwise. Equal probabilities for anything between \(a\) and \(b\), otherwise \(0\).
%
\item No real need for CDF. Use rectangle intuition: the height is \(\frac{1}{b-a}\) and the width would be the amount `along' the rectangle you would be. \(P(a \leq X \leq c) = (c-a) \cdot \frac{1}{b-a} = P(X \leq c)\) if the probability is equal between \(a\) and \(b\). If the density is high, the CDF's graph is steep.
%
\item Normal (Gaussian): \(X \sim N(\mu, \sigma)\) if \(f(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \cdot e^{-(x-\mu)^2 / (2\sigma^2)}\). \(\mu\) is the centre while \(\sigma\) measures how widely spread it is. Height, sheep producing wool, etc., where many random factors are involved.
%
\item About \(\frac{2}{3}\) of probability mass is within one SD of the mean, 95\% within two.
%
\item Standard normal if \(\mu=0\) and \(\sigma=1\). \(f(z; 0,1) = \frac{1}{\sqrt{2\pi}} \cdot e^{-z^2 / 2}\).
%
\item Standardising: If \(X \sim N(\mu, \sigma)\), \(Z=\frac{X-\mu}{\sigma} \mid Z \sim N(0,1)\). \(P(X \leq a) = P(Z \leq \frac{a-\mu}{\sigma}) = \Phi(\frac{a-\mu}{\sigma})\), \(\eta_p = \mu + \Phi^{-1}(p) \cdot \sigma\). Go opposite way in table for inverse.
%
\item Approximating binomial: find \(\mu = np\) and \(\sigma = \sqrt{npq}\) where \(q = (1-p)\). Use these two values as parameters for normal. Thus \(P(X \leq x) = \Phi(\frac{x + 0.5 - np}{\sqrt{npq}})\). Adequate if \(np,nq\geq 10\). We add \(0.5\) for continuity correction.
%
\item Exponential: how long until something happens, with no memory: \(X \sim \operatorname{Exp}(\lambda)\) if \(f(x; \lambda) = \lambda e^{-\lambda x}\) if \(x > 0\), \(0\) otherwise. \(F(x; \lambda) = 1-e^{-\lambda x}\) if \(x > 0\), \(0\) otherwise.
%
\item \(\mathbb{E}(X) = \frac{1}{\lambda} = \operatorname{SD}(X)\). \(\operatorname{Var}(X) = \frac{1}{\lambda^2}\).
%
\item Relation to Poisson: Poisson counts the number of arrivals each minute, while exponential counts the time between arrivals at a drive-through.
\end{itemize}

\subsection*{Transformations of Random Variables}

\begin{itemize}
	\item \textbf{Linearity of \(\mathbb{E}\)}: expectation of the sum of RVs is the sum of their expectations. \textbf{Rescaling}: \(\mathbb{E}(aX + b)=a \mathbb{E}(X) + b\), \(\operatorname{Var}(aX + b) = a^2 \operatorname{Var}(X)\), \(\operatorname{SD}(aX+b) = |a|\operatorname{SD}(X)\).
	%
	\item \(\mathbb{E}(h(X)) = \int_{-\infty}^{\infty} h(x) \cdot f(x)\,dx\).
	%
	\item \textbf{Transformations of RVs are themselves RVs}: if a transformation of a random variable \(Y=g(X)\) is monotonically increasing, like radius to area, then CDF is \(F_Y(y)=F_X(g^{-1}(y))\). If monotonically decreasing, like speed to time, then \(F_Y(y)=1 - F_X(g^{-1}(y))\) since the inequality is flipped.
	%
	\item PDF is \(f_Y(y) = f_X(g^{-1}(y)) \cdot |\frac{d}{dy} g^{-1}(y)|\), where the derivative accounts for the change in width of the curve.
\end{itemize}

\subsection*{Joint Probability Distributions}
